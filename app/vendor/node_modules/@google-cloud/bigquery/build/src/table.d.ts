/*!
 * Copyright 2014 Google Inc. All Rights Reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
/// <reference types="node" />
import * as common from '@google-cloud/common';
import * as r from 'request';
import { BigQuery, Job, Dataset, Query, SimpleQueryRowsResponse, SimpleQueryRowsCallback } from '../src';
import { Duplex, Readable, Writable } from 'stream';
import { JobMetadata } from './job';
export interface File {
    bucket: any;
    kmsKeyName?: string;
    userProject?: string;
    name: string;
    generation?: number;
}
export interface JobMetadataCallback {
    (err: Error | null, metadataOrResponse: JobMetadata | r.Response): void;
}
export declare type JobMetadataResponse = [JobMetadata];
export declare type RowMetadata = any;
export interface InsertRowsOptions {
    autoCreate?: boolean;
    ignoreUnknownValues?: boolean;
    raw?: boolean;
    schema?: string | {};
    skipInvalidRows?: boolean;
    templateSuffix?: string;
}
export declare type RowsResponse = [RowMetadata[], GetRowsOptions, r.Response];
export interface RowsCallback {
    (err: Error | null, rows?: RowMetadata[] | null, nextQuery?: GetRowsOptions | null, apiResponse?: r.Response): void;
}
export interface TableRow {
    f: TableRowField[];
}
export interface TableRowField {
    v: string | TableRow | TableRowField[];
}
export declare type TableRowValue = string | TableRow;
export interface GetRowsOptions {
    startIndex?: number;
    selectedFields?: string;
    autoPaginate?: boolean;
    maxApiCalls?: number;
    maxResults?: number;
}
export interface JobLoadMetadata {
    jobId?: string;
    jobPrefix?: string;
    allowJaggedRows?: boolean;
    allowQuotedNewlines?: boolean;
    autodetect?: boolean;
    clustering?: {
        fields: number[];
    };
    createDisposition?: string;
    destinationEncryptionConfiguration?: {
        kmsKeyName?: string;
    };
    destinationTable?: {
        datasetId: string;
        projectId: string;
        tableId: string;
    };
    destinationTableProperties?: {
        description?: string;
        friendlyName?: string;
    };
    encoding?: string;
    fieldDelimiter?: string;
    ignoreUnknownValues?: boolean;
    maxBadRecords?: number;
    nullMarker?: string;
    projectionFields?: string[];
    quote?: string;
    schema?: {
        fields: TableField[];
    };
    schemaUpdateOptions?: string[];
    skipLeadingRows?: number;
    sourceFormat?: string;
    format?: string;
    location?: string;
    sourceUris?: string[];
    timePartitioning?: {
        expirationMs?: number;
        field?: string;
        requirePartitionFilter?: boolean;
        type?: string;
    };
    writeDisposition?: string;
}
export interface CreateExtractJobOptions {
    format?: 'CSV' | 'JSON' | 'AVRO' | 'PARQUET' | 'ORC';
    gzip?: boolean;
    jobId?: string;
    jobPrefix?: string;
    destinationFormat?: string;
    compression?: string;
}
export declare type JobResponse = [Job, r.Response];
export interface JobCallback {
    (err: Error | null, job?: Job | null, apiResponse?: r.Response): void;
}
export interface CreateCopyJobMetadata extends CopyTableMetadata {
    destinationTable?: {
        datasetId: string;
        projectId: string;
        tableId: string;
    };
    sourceTable?: {
        datasetId: string;
        projectId: string;
        tableId: string;
    };
    sourceTables: Array<{
        datasetId: string;
        projectId: string;
        tableId: string;
    }>;
}
export interface SetTableMetadataOptions {
    description?: string;
    schema?: string | {};
}
export interface CopyTableMetadata {
    jobId?: string;
    jobPrefix?: string;
    createDisposition?: 'CREATE_IF_NEEDED' | 'CREATE_NEVER';
    writeDisposition?: 'WRITE_TRUNCATE' | 'WRITE_APPEND' | 'WRITE_EMPTY';
    destinationEncryptionConfiguration?: {
        kmsKeyName?: string;
    };
}
export interface TableMetadata {
    name?: string;
    friendlyName: string;
    schema?: string | TableField[];
    partitioning?: string;
    view?: string | ViewDefinition;
}
export interface ViewDefinition {
    query: string;
    useLegacySql?: boolean;
}
export interface FormattedMetadata {
    schema?: TableSchema;
    friendlyName: string;
    name?: string;
    partitioning?: string;
    timePartitioning?: {
        type: string;
    };
    view: ViewDefinition;
}
export interface TableSchema {
    fields: TableField[];
}
export interface TableField {
    name: string;
    type: string;
    mode?: string;
    fields?: TableField[];
}
export declare type ApiResponse = [r.Response];
export interface ApiResponseCallback {
    (err: Error | null, apiResponse?: r.Response): void;
}
export interface TableOptions {
    location?: string;
}
/**
 * Table objects are returned by methods such as
 * {@link Dataset#table}, {@link Dataset#createTable}, and
 * {@link Dataset#getTables}.
 *
 * @class
 * @param {Dataset} dataset {@link Dataset} instance.
 * @param {string} id The ID of the table.
 * @param {object} [options] Table options.
 * @param {string} [options.location] The geographic location of the table, by
 *      default this value is inherited from the dataset. This can be used to
 *      configure the location of all jobs created through a table instance. It
 *      cannot be used to set the actual location of the table. This value will
 *      be superseded by any API responses containing location data for the
 *      table.
 *
 * @example
 * const {BigQuery} = require('@google-cloud/bigquery');
 * const bigquery = new BigQuery();
 * const dataset = bigquery.dataset('my-dataset');
 *
 * const table = dataset.table('my-table');
 */
declare class Table extends common.ServiceObject {
    dataset: Dataset;
    bigQuery: BigQuery;
    location?: string;
    createReadStream: (options?: GetRowsOptions) => Readable;
    constructor(dataset: Dataset, id: string, options?: TableOptions);
    /**
     * Convert a comma-separated name:type string to a table schema object.
     *
     * @static
     * @private
     *
     * @param {string} str Comma-separated schema string.
     * @returns {object} Table schema in the format the API expects.
     */
    static createSchemaFromString_(str: string): TableSchema;
    /**
     * Convert a row entry from native types to their encoded types that the API
     * expects.
     *
     * @static
     * @private
     *
     * @param {*} value The value to be converted.
     * @returns {*} The converted value.
     */
    static encodeValue_(value?: {} | null): {} | null;
    /**
     * @private
     */
    static formatMetadata_(options: TableMetadata): FormattedMetadata;
    copy(destination: Table, metadata?: CopyTableMetadata): Promise<JobMetadataResponse>;
    copy(destination: Table, metadata: CopyTableMetadata, callback: JobMetadataCallback): void;
    copy(destination: Table, callback: JobMetadataCallback): void;
    copyFrom(sourceTables: Table | Table[], metadata?: CopyTableMetadata): Promise<JobMetadataResponse>;
    copyFrom(sourceTables: Table | Table[], metadata: CopyTableMetadata, callback: JobMetadataCallback): void;
    copyFrom(sourceTables: Table | Table[], callback: JobMetadataCallback): void;
    createCopyJob(destination: Table, metadata?: CreateCopyJobMetadata): Promise<JobResponse>;
    createCopyJob(destination: Table, metadata: CreateCopyJobMetadata, callback: JobCallback): void;
    createCopyJob(destination: Table, callback: JobCallback): void;
    createCopyFromJob(source: Table | Table[], metadata?: CopyTableMetadata): Promise<JobResponse>;
    createCopyFromJob(source: Table | Table[], metadata: CopyTableMetadata, callback: JobCallback): void;
    createCopyFromJob(source: Table | Table[], callback: JobCallback): void;
    createExtractJob(destination: File, options?: CreateExtractJobOptions): Promise<JobResponse>;
    createExtractJob(destination: File, options: CreateExtractJobOptions, callback: JobCallback): void;
    createExtractJob(destination: File, callback: JobCallback): void;
    createLoadJob(source: string, metadata?: JobLoadMetadata): Writable;
    createLoadJob(source: File, metadata?: JobLoadMetadata): Promise<JobResponse>;
    createLoadJob(source: string, metadata: JobLoadMetadata, callback: JobCallback): Writable;
    createLoadJob(source: File, metadata: JobLoadMetadata, callback: JobCallback): void;
    createLoadJob(source: string, callback: JobCallback): Writable;
    createLoadJob(source: File, callback: JobCallback): void;
    createQueryJob(options: Query): Promise<JobResponse>;
    createQueryJob(options: Query, callback: JobCallback): void;
    /**
     * Run a query scoped to your dataset as a readable object stream.
     *
     * See {@link BigQuery#createQueryStream} for full documentation of this
     * method.
     *
     * @param {object} query See {@link BigQuery#createQueryStream} for full
     *     documentation of this method.
     * @returns {stream} See {@link BigQuery#createQueryStream} for full
     *     documentation of this method.
     */
    createQueryStream(query: Query): Duplex;
    /**
     * Creates a write stream. Unlike the public version, this will not
     * automatically poll the underlying job.
     *
     * @private
     *
     * @param {string|object} [metadata] Metadata to set with the load operation.
     *     The metadata object should be in the format of the
     *     [`configuration.load`](http://goo.gl/BVcXk4) property of a Jobs
     * resource. If a string is given, it will be used as the filetype.
     * @param {string} [metadata.jobId] Custom job id.
     * @param {string} [metadata.jobPrefix] Prefix to apply to the job id.
     * @returns {WritableStream}
     */
    createWriteStream_(metadata: JobLoadMetadata | string): Writable;
    /**
     * Load data into your table from a readable stream of AVRO, CSV, JSON, ORC,
     * or PARQUET data.
     *
     * @see [Jobs: insert API Documentation]{@link https://cloud.google.com/bigquery/docs/reference/v2/jobs/insert}
     *
     * @param {string|object} [metadata] Metadata to set with the load operation.
     *     The metadata object should be in the format of the
     *     [`configuration.load`](http://goo.gl/BVcXk4) property of a Jobs
     * resource. If a string is given, it will be used as the filetype.
     * @param {string} [metadata.jobId] Custom job id.
     * @param {string} [metadata.jobPrefix] Prefix to apply to the job id.
     * @returns {WritableStream}
     *
     * @throws {Error} If source format isn't recognized.
     *
     * @example
     * const {BigQuery} = require('@google-cloud/bigquery');
     * const bigquery = new BigQuery();
     * const dataset = bigquery.dataset('my-dataset');
     * const table = dataset.table('my-table');
     *
     * //-
     * // Load data from a CSV file.
     * //-
     * const request = require('request');
     *
     * const csvUrl = 'http://goo.gl/kSE7z6';
     *
     * const metadata = {
     *   allowJaggedRows: true,
     *   skipLeadingRows: 1
     * };
     *
     * request.get(csvUrl)
     *   .pipe(table.createWriteStream(metadata))
     *   .on('job', (job) => {
     *     // `job` is a Job object that can be used to check the status of the
     *     // request.
     *   })
     *   .on('complete', (job) => {
     *     // The job has completed successfully.
     *   });
     *
     * //-
     * // Load data from a JSON file.
     * //-
     * const fs = require('fs');
     *
     * fs.createReadStream('./test/testdata/testfile.json')
     *   .pipe(table.createWriteStream('json'))
     *   .on('job', (job) => {
     *     // `job` is a Job object that can be used to check the status of the
     *     // request.
     *   })
     *   .on('complete', (job) => {
     *     // The job has completed successfully.
     *   });
     */
    createWriteStream(metadata: JobLoadMetadata | string): Writable;
    extract(destination: File, options?: CreateExtractJobOptions): Promise<JobMetadataResponse>;
    extract(destination: File, options: CreateExtractJobOptions, callback?: JobMetadataCallback): void;
    extract(destination: File, callback?: JobMetadataCallback): void;
    getRows(options?: GetRowsOptions): Promise<RowsResponse>;
    getRows(options: GetRowsOptions, callback: RowsCallback): void;
    getRows(callback: RowsCallback): void;
    insert(rows: RowMetadata | RowMetadata[], options?: InsertRowsOptions): Promise<ApiResponse>;
    insert(rows: RowMetadata | RowMetadata[], options: InsertRowsOptions, callback: ApiResponseCallback): void;
    insert(rows: RowMetadata | RowMetadata[], callback: ApiResponseCallback): void;
    load(source: string | File, metadata?: JobLoadMetadata): Promise<JobMetadataResponse>;
    load(source: string | File, metadata: JobLoadMetadata, callback: JobMetadataCallback): void;
    load(source: string | File, callback: JobMetadataCallback): void;
    query(query: Query): Promise<SimpleQueryRowsResponse>;
    query(query: Query, callback: SimpleQueryRowsCallback): void;
    setMetadata(metadata: SetTableMetadataOptions): Promise<common.SetMetadataResponse>;
    setMetadata(metadata: SetTableMetadataOptions, callback: common.ResponseCallback): void;
}
/**
 * Reference to the {@link Table} class.
 * @name module:@google-cloud/bigquery.Table
 * @see Table
 */
export { Table };
